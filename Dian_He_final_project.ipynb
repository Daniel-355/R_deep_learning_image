{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Convolutional Neural Network to classify pictures of pathology of colorectal cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the below coding can be run by Pycharm well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: \n",
    "* To build a my own CNN (Convolutional Neural Network) to classify two kinds of colorectal cancer in case it can help clinical practitioners to classify the two kinds of colorectal cancer more accurately. \n",
    "please read the [outline](https://github.com/Daniel-355/R_deep_learning_image/blob/master/My_project.md) to know more detail "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sources  \n",
    "These pictures have been labeled by pathologists at the University Hospitals Coventry and Warwickshire [link](https://warwick.ac.uk/fac/sci/dcs/research/tia/glascontest/download/), which included two classifications: benign (74) and malignant (91). All 165 pictures were split into two datasets: training dataset (85) and test dataset (80). \n",
    "The composition of the dataset is as follows.  \n",
    "   \n",
    "     \n",
    "Split\t|Warwick-QU\n",
    "----|----\n",
    "Training\t| benign : 37\n",
    "||malignant : 48\n",
    "Test\t|benign : 37\n",
    "||malignant : 43  \n",
    "\n",
    "Tabel of classification of colorectal cancer  \n",
    "![Tabel of classification of colorectal cancer](https://github.com/Daniel-355/R_deep_learning_image/blob/master/Picture1.png?raw=true)  \n",
    " \n",
    "Sample of picture  \n",
    "![Sample of picture](https://github.com/Daniel-355/R_deep_learning_image/blob/master/Picture2.png?raw=true )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "Use \"convert\" function to transform all pictures to \"tensorflow\" data automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load general use libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.python.keras.layers import Input, Dense\n",
    "from tensorflow.python.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Dropout, Flatten, Dense\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, Input, BatchNormalization, ReLU\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers.core import Flatten, Activation, Dense, Dropout\n",
    "from tensorflow.python.keras.layers.core import ActivityRegularization\n",
    "from tensorflow.python.keras.layers.advanced_activations import LeakyReLU\n",
    "#\n",
    "from tensorflow.python.keras.layers.noise import GaussianNoise\n",
    "# layer = GaussianNoise(0.2)\n",
    "# from keras.applications.inception_v3 import InceptionV3\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "from tensorflow.python.keras.layers import Input, Dense\n",
    "from tensorflow.python.keras import Model\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import initializers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.1.0\n",
    "# !pip install Keras==2.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Load Datasets-----------------\n"
     ]
    }
   ],
   "source": [
    "# set directary\n",
    "# did not split traning dataset and test dataset here\n",
    "path1=\"C:/Users/hed2/Downloads/MOOCTF/class4/class4/FASHION_FC/\"\n",
    "\n",
    "train_path = path1 + 'fashion_image_label/fashion_train_jpg_60000/' # pictures directory\n",
    "train_txt = path1 +'fashion_image_label/fashion_train_jpg_60000.txt' # pictures corresponded labels\n",
    "x_train_savepath = path1 +'fashion_image_label/fashion_x_train.npy' # data features path\n",
    "y_train_savepath = path1 +'fashion_image_label/fahion_y_train.npy' # data label paht\n",
    "\n",
    "# define a function named \"generateds\" to read pictures into \"tf\" data one by one\n",
    "def generateds(path, txt):\n",
    "    f = open(txt, 'r')\n",
    "    contents = f.readlines()  \n",
    "    f.close()\n",
    "    x, y_ = [], []\n",
    "    for content in contents:\n",
    "        value = content.split()   \n",
    "        img_path = path + value[0]\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((500, 350))\n",
    "        img = np.array(img.convert('RGB'))\n",
    "#         convert pictures to RGB with 3 levels\n",
    "        img = img / 255.\n",
    "        x.append(img)\n",
    "        y_.append(value[1])\n",
    "        print('loading : ' + content)\n",
    "    x = np.array(x)\n",
    "    y_ = np.array(y_)\n",
    "    y_ = y_.astype(np.int64)\n",
    "    return x, y_\n",
    "\n",
    "\n",
    "if os.path.exists(x_train_savepath) and os.path.exists(y_train_savepath):\n",
    "    print('-------------Load Datasets-----------------')\n",
    "    x_train_save = np.load(x_train_savepath, allow_pickle = True)\n",
    "    y_train = np.load(y_train_savepath, allow_pickle = True)\n",
    "    x_train = np.reshape(x_train_save, (len(x_train_save), 350, 500, 3))\n",
    " \n",
    "    # save transformed data as training data and test data\n",
    "else:\n",
    "    print('-------------Generate Datasets-----------------')\n",
    "    x_train, y_train = generateds(train_path, train_txt)\n",
    "   \n",
    "    print('-------------Save Datasets-----------------')\n",
    "    x_train_save = np.reshape(x_train, (len(x_train), -1))\n",
    " \n",
    "    np.save(x_train_savepath, x_train_save)\n",
    "    np.save(y_train_savepath, y_train)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training dataset and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_train\n",
    "y_data = y_train\n",
    "\n",
    "np.random.seed(123)  #  shuffle data\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(y_data)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "#  define x_train, y_train, x_test and y_test\n",
    "x_train = x_data[:-30]\n",
    "y_train = y_data[:-30]\n",
    "x_test = x_data[-30:]\n",
    "y_test = y_data[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmenation\n",
    "* Only for x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen_train = ImageDataGenerator(\n",
    "    # rescale=1. / 1.,   \n",
    "    rotation_range=15,   \n",
    "    width_shift_range=.15,   \n",
    "    height_shift_range=.15,   \n",
    "    horizontal_flip=True,  \n",
    "    vertical_flip=True,   \n",
    "    zoom_range=0.8   \n",
    ")\n",
    "image_gen_train.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint: store the traing weight values\n",
    "checkpoint_save_path = \"C:/Users/hed2/Downloads/MOOCTF/class4/class4/FASHION_FC/fashion_image_label/fashion2.h5\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_best_only=True, save_weights_only=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build traning structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: Radiomics - 1p19q Chromosome Status Classification\n",
    "def simpleCNN():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3, 3), activation='linear', kernel_initializer='he_uniform', input_shape=(350, 500, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())  # add an advanced activation\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='linear', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())  # add an advanced activation\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='linear', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())  # add an advanced activation\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='linear', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())  # add an advanced activation\n",
    "    model.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "\n",
    "    model.add(GaussianNoise(0.2))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    model.add(ReLU())  # add an advanced activation\n",
    "\n",
    "    model.add(Dropout(0.25))  # Avoid over-fitting\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using weight initializer \n",
    "CNNpretrain = simpleCNN()\n",
    "CNNpretrain.load_weights(\"C:/Users/hed2/Downloads/MOOCTF/class4/class4/FASHION_FC/fashion_image_label/fashion1.h5\")\n",
    "model=CNNpretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model.complile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam_m = keras.optimizers.Adam(lr=0.0005)\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(image_gen_train.flow(x_train, y_train, batch_size=8), epochs=40, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw loss and acc plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['sparse_categorical_accuracy']\n",
    "val_acc = history.history['val_sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.ylim([0, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![output](https://github.com/Daniel-355/R_deep_learning_image/blob/master/Figure_fianl.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "x_predict = x_test[0][tf.newaxis, ...]\n",
    "result = model.predict(x_predict)\n",
    "pred = tf.argmax(result, axis=1)\n",
    "tf.print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks for your review and your questions are welcomed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
